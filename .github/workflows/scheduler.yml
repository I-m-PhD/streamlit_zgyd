# .github/workflows/scheduler.yml

name: Scheduled Crawler

on:
  workflow_dispatch:
    inputs:
      task_to_run:
        description: 'Scheduled Task ID (TASK_1, TASK_2, or TASK_3) or Manual Run'
        required: true
        type: choice
        default: 'TASK_1'
        options:
          - TASK_1
          - TASK_2
          - TASK_3

permissions:
  contents: write # 授予写入权限用于提交数据 (用于 PyGithub commit 和 git-auto-commit-action)

jobs:
  # T1: 任务 1 Job 定义
  run_task_1:
    name: "TASK 1: 所有招采"
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.task_to_run == 'TASK_1'
    steps:
      - name: Checkout code for Local Action Access
        uses: actions/checkout@v5
      - name: Execute Composite Action for TASK_1
        uses: ./.github/actions/sop
        with:
          task_name: TASK_1

  # T2: 任务 2 Job 定义
  run_task_2:
    name: "TASK 2: 所有招采_正在招标"
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.task_to_run == 'TASK_2'
    steps:
      - name: Checkout code for Local Action Access
        uses: actions/checkout@v5
      - name: Execute Composite Action for TASK_2
        uses: ./.github/actions/sop
        with:
          task_name: TASK_2

  # T3: 任务 3 Job 定义 (重构为差异化推送)
  run_task_3:
    name: "TASK 3: 所有招采_正在招标_北京"
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.task_to_run == 'TASK_3'
      
    steps:
      - name: Checkout code (Fetch Full History for PyGithub)
        uses: actions/checkout@v5
        with:
          fetch-depth: 0 # 允许 PyGithub 获取旧状态并提交新状态

      - name: Setup Python and Install dependencies
        # 重用 sop Action 的 Python 设置和依赖安装步骤
        # 只执行 setup-python 和 pip install -r requirements.txt
        uses: ./.github/actions/sop
        with:
          task_name: DUMMY # 传递一个虚拟任务名以满足输入要求，但不会执行其最终的 git-auto-commit

      - name: Run Crawler, Compare, and Push (TASK 3)
        shell: bash
        run: python crawler.py TASK_3
        env:
          # 注入 Secrets 供 crawler.py 访问
          CLOUDFLARE_WORKER: ${{ secrets.CLOUDFLARE_WORKER }} # GitHub PAT
          WECHAT_WEBHOOK_URL: ${{ secrets.WECHAT_WEBHOOK_URL }} # Server Chan API URL
          GITHUB_OWNER: ${{ github.repository_owner }} 
          GITHUB_REPO: ${{ github.event.repository.name }}

      - name: Commit and Push Local Data Files (for Streamlit)
        # 提交 Streamlit 需要的 zgyd 目录文件。
        # 现在 task_3_state.json 也是在本地创建的，必须明确包含在提交模式中。
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "SCHEDULER: Auto-update data [TASK_3] for Streamlit."
          file_pattern: 'zgyd/*.json zgyd/metadata.json zgyd/task_3_state.json'
          commit_user_name: 'github-actions[bot]'
          commit_user_email: 'github-actions[bot]@users.noreply.github.com'
          push_options: '--force-with-lease'