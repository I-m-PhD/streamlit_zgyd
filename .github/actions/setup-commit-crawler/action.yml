name: 'Setup, Run Crawler, and Commit Data'
description: 'Sets up environment, runs the specified crawler task, and commits the results.'

# 定义输入参数：我们只接收一个 task_name，如 TASK_1, TASK_2, TASK_3
inputs:
  task_name:
    description: 'The name of the crawler task to run (e.g., TASK_1)'
    required: true

runs:
  using: "composite"
  steps:
    # 1. Checkout code
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        # 必须使用 token，因为后续需要推送到仓库
        token: ${{ github.token }}

    # 2. Set up Python and Install dependencies (假设 requirements.txt 在根目录)
    - name: Set up Python and Install dependencies
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      shell: bash
      run: pip install -r requirements.txt

    # 3. 最佳实践：时区修正
    - name: Set Timezone to Asia/Shanghai (CST)
      shell: bash
      run: echo "TZ=Asia/Shanghai" >> $GITHUB_ENV

    # 4. 核心：运行爬虫
    - name: Run Crawler for ${{ inputs.task_name }}
      shell: bash
      # 注意：我们必须在 run 之前设置 shell，这是 Composite Action 的要求
      run: python crawler.py ${{ inputs.task_name }}

    # 5. 最佳实践：原子性提交（使用 push-options 处理并发）
    - name: Commit and Push new data
      uses: stefanzweifel/git-auto-commit-action@v5
      with:
        commit_message: "SCHEDULER: Auto-update data [${{ inputs.task_name }}]."
        file_pattern: 'zgyd/*.json zgyd/metadata.json'
        commit_user_name: 'github-actions[bot]'
        commit_user_email: 'github-actions[bot]@users.noreply.github.com'
        # 关键修复：允许拉取并合并，解决 non-fast-forward 错误
        push_options: '--force-with-lease'