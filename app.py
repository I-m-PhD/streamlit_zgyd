# app.py

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.io as pio
import json
import os

# --- CONFIGURATION ---
OUTPUT_DIR = "./zgyd"
METADATA_PATH = os.path.join(OUTPUT_DIR, "metadata.json")

# å®šä¹‰æ‰€æœ‰éœ€è¦é‡‡é›†çš„ä»»åŠ¡é…ç½® (ç”¨äºè¯†åˆ«æ–‡ä»¶å)
TASK_CONFIG = {
    "TASK_1": {"payload": {}, "name": "æ‰€æœ‰æ‹›é‡‡"},
    "TASK_2": {"payload": {"homePageQueryType": "Bidding"}, "name": "æ‰€æœ‰æ‹›é‡‡_æ­£åœ¨æ‹›æ ‡"},
    "TASK_3": {"payload": {"homePageQueryType": "Bidding", "companyType": "BJ"}, "name": "æ‰€æœ‰æ‹›é‡‡_æ­£åœ¨æ‹›æ ‡_åŒ—äº¬"},
}

# --- METADATA AND DATA LOADING ---

def load_metadata():
    """Loads the last successful crawl time for all tasks."""
    if os.path.exists(METADATA_PATH):
        try:
            with open(METADATA_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def load_data(task_name):
    """Loads data from a local JSON file."""
    output_path = os.path.join(OUTPUT_DIR, f"{task_name}.json")
    if os.path.exists(output_path):
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return None
    return None

# --- DATA ANALYSIS AND PLOTTING FUNCTION ---

# Setting plotly template
pio.templates.default = "plotly_dark"

def show_statistics(all_content, data_name, crawl_time):

    st.markdown("---")
    st.header(f"æ•°æ®åˆ†æ: {data_name}")

    # è·å–è®°å½•æ€»æ•°
    record_count = len(all_content) if all_content is not None else 0

    # ---------------------------------------------------------
    # å…³é”®ä¿®æ”¹ï¼šæ˜¾ç¤ºé‡‡é›†çŠ¶æ€å’Œæ•°æ®æ¡æ•°
    # ---------------------------------------------------------
    col_time, col_count = st.columns([2, 1])

    with col_time:
        st.subheader("é‡‡é›†çŠ¶æ€")
        if crawl_time:
            st.caption(f"ä¸Šæ¬¡é‡‡é›†æ—¶é—´ï¼š**{crawl_time}** (ç”± GitHub Action å®šæ—¶æ›´æ–°)")
        else:
            st.caption("ä¸Šæ¬¡é‡‡é›†æ—¶é—´ï¼š**æ— è®°å½•** (ç”± GitHub Action å®šæ—¶æ›´æ–°)")

    with col_count:
        st.subheader("æ•°æ®é‡")
        if record_count > 0:
            st.caption(f"å½“å‰æ€»è®°å½•æ•°ï¼š**{record_count}** æ¡")
        else:
            st.caption("å½“å‰æ€»è®°å½•æ•°ï¼š**0** æ¡ (ç­‰å¾…é¦–æ¬¡é‡‡é›†)")

    st.markdown("---")
    # ---------------------------------------------------------

    if not all_content:
        st.warning("æ— æ•°æ®å¯ä¾›åˆ†æã€‚")
        return

    df = pd.DataFrame(all_content)

    # ä¿®å¤äº† locale.Error çš„ä»£ç 
    if df.empty or 'publishDate' not in df.columns:
        st.warning("DataFrame ä¸ºç©ºæˆ–ç¼ºå°‘ 'publishDate' å­—æ®µã€‚æ— æ³•ç”Ÿæˆå›¾è¡¨ã€‚")
        return

    try:
        df['PublishDateTime'] = pd.to_datetime(df['publishDate'])
    except Exception as e:
        st.error(f"æ—¥æœŸæ ¼å¼è½¬æ¢é”™è¯¯: {e}")
        return

    cutoff_date = pd.to_datetime('2024-01-01')
    initial_count = len(df)
    df = df[df['PublishDateTime'] >= cutoff_date].copy()
    filtered_count = len(df)

    if initial_count != filtered_count:
        st.info(f"å·²è¿‡æ»¤ {initial_count - filtered_count} æ¡æ—©äº {cutoff_date.date()} çš„å†å²å™ªéŸ³è®°å½•ã€‚")

    df['PublishDateOnly'] = df['PublishDateTime'].dt.date
    df['PublishHour'] = df['PublishDateTime'].dt.hour

    # ä½¿ç”¨ä¸å¸¦ locale å‚æ•°çš„æ–¹æ³•è·å–è‹±æ–‡åç§°
    df['PublishDayOfWeek'] = df['PublishDateTime'].dt.day_name()

    # ä¸­æ–‡æ˜ å°„é€»è¾‘
    day_map = {'Monday': 'å‘¨ä¸€', 'Tuesday': 'å‘¨äºŒ', 'Wednesday': 'å‘¨ä¸‰', 'Thursday': 'å‘¨å››', 'Friday': 'å‘¨äº”', 'Saturday': 'å‘¨å…­', 'Sunday': 'å‘¨æ—¥'}
    df['PublishDayOfWeek'] = df['PublishDayOfWeek'].map(day_map)
    day_order = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
    df['PublishDayOfWeek'] = pd.Categorical(df['PublishDayOfWeek'], categories=day_order, ordered=True)

    # --- Plotting Logic (ä¿æŒä¸å˜) ---
    st.subheader("1. æ¯æ—¥æ›´æ–°é¢‘æ¬¡")
    frequency_df = df.groupby(['PublishDateOnly', 'PublishDayOfWeek'], observed=True)['PublishDateTime'].count().reset_index()
    frequency_df.columns = ['PublishDate', 'PublishDayOfWeek', 'UpdateCount']
    frequency_df = frequency_df.sort_values('PublishDate')
    fig_freq = px.bar(frequency_df, x='PublishDate', y='UpdateCount', title='æ¯æ—¥æ›´æ–°é¢‘æ¬¡ (å¯ç¼©æ”¾)', labels={'UpdateCount': 'æ›´æ–°é¢‘æ¬¡', 'PublishDate': 'æ—¥æœŸ', 'PublishDayOfWeek': 'å‘¨å‡ '}, hover_data=['PublishDayOfWeek'], height=500)
    fig_freq.update_xaxes(tickangle=-45, rangeslider_visible=True, rangeselector=dict(bgcolor="#333333", activecolor="#555555", font=dict(color="white"), buttons=[dict(count=7, label="1å‘¨", step="day", stepmode="backward"), dict(count=1, label="1æœˆ", step="month", stepmode="backward"), dict(count=3, label="1å­£", step="month", stepmode="backward"), dict(count=1, label="1å¹´", step="year", stepmode="backward"), dict(step="all", label="å…¨éƒ¨")]), tickformat="%Y-%m-%d")
    st.plotly_chart(fig_freq, use_container_width=True)

    st.subheader("2. æ›´æ–°æ´»è·ƒåº¦åˆ†æ")
    col1, col2 = st.columns(2)
    with col1:
        time_df_hour = df.groupby('PublishHour', observed=True)['PublishDateTime'].count().reset_index(name='UpdateCount')
        fig_hour = px.bar(time_df_hour, x='PublishHour', y='UpdateCount', title='å…¨æ—¶æ®µæ›´æ–°æ´»è·ƒåº¦', labels={'PublishHour': 'æ—¶åˆ» (0-23)', 'UpdateCount': 'æ›´æ–°é¢‘æ¬¡'}, height=400)
        fig_hour.update_layout(xaxis={'tickmode': 'linear', 'dtick': 1})
        st.plotly_chart(fig_hour, use_container_width=True)
    with col2:
        hour_order = list(range(24))
        time_df_heatmap = df.groupby(['PublishHour', 'PublishDayOfWeek'], observed=True).size().reset_index(name='UpdateCount')
        index = pd.MultiIndex.from_product([hour_order, day_order], names=['PublishHour', 'PublishDayOfWeek'])
        time_df_heatmap = time_df_heatmap.set_index(['PublishHour', 'PublishDayOfWeek']).reindex(index, fill_value=0).reset_index()
        time_df_heatmap['PublishDayOfWeek'] = pd.Categorical(time_df_heatmap['PublishDayOfWeek'], categories=day_order, ordered=True)
        fig_heatmap = px.density_heatmap(time_df_heatmap, x="PublishHour", y="PublishDayOfWeek", z="UpdateCount", title='æ›´æ–°æ´»è·ƒåº¦: æ—¶åˆ» vs. å‘¨å‡ ', labels={"PublishHour": "æ—¶åˆ»", "PublishDayOfWeek": "å‘¨å‡ ", "UpdateCount": "æ›´æ–°é¢‘æ¬¡"}, category_orders={"PublishDayOfWeek": day_order, "PublishHour": hour_order}, nbinsx=24, color_continuous_scale=px.colors.sequential.Viridis, height=400)
        fig_heatmap.update_xaxes(range=[-0.5, 23.5], tickmode='linear', dtick=1)
        st.plotly_chart(fig_heatmap, use_container_width=True)


    # --- 3. åŸå§‹æ•°æ®è¡¨æ ¼ (ä»…é™åŒ—äº¬) ---
    if data_name == "æ‰€æœ‰æ‹›é‡‡_æ­£åœ¨æ‹›æ ‡_åŒ—äº¬":
        st.subheader("3. åŸå§‹æ•°æ®è¡¨")

        required_cols_map = {
            'publishDate': 'å‘å¸ƒæ—¶é—´',
            'companyTypeName': 'å…¬å¸åŒºåŸŸ',
            'name': 'æ ‡é¢˜',
            'tenderSaleDeadline': 'æˆªæ ‡æ—¶é—´',
            'backDate': 'é€€å›æ—¶é—´'
        }

        available_cols = [col for col in required_cols_map.keys() if col in df.columns]

        if not available_cols:
            st.warning("æ— æ³•æ˜¾ç¤ºæ•°æ®è¡¨ï¼šæŠ“å–çš„æ•°æ®ä¸­ç¼ºå°‘å¿…è¦çš„å­—æ®µã€‚")
            return

        rename_map = {col: required_cols_map[col] for col in available_cols}
        display_df = df[available_cols].rename(columns=rename_map)

        if 'å‘å¸ƒæ—¶é—´' in display_df.columns:
            display_df = display_df.sort_values(by='å‘å¸ƒæ—¶é—´', ascending=False)

        st.dataframe(display_df, use_container_width=True, height=600)


# --- MAIN APPLICATION ENTRY POINT ---

def main():
    st.set_page_config(
        page_title="æ‹›é‡‡æ•°æ®ç›‘æ§",
        layout="wide",
        initial_sidebar_state="collapsed"
    )

    st.title("ğŸ“¡ ä¸­å›½ç§»åŠ¨æ‹›é‡‡å¹³å°æ•°æ®ç›‘æ§")
    st.info("æ‰€æœ‰æ•°æ®é›†å‡é€šè¿‡ GitHub Actions å®šæ—¶æ›´æ–°ã€‚")

    metadata = load_metadata()

    # éå†æ‰€æœ‰ä»»åŠ¡é…ç½®
    for task_key in TASK_CONFIG.keys():
        config = TASK_CONFIG[task_key]
        task_name = config["name"]

        crawl_time = metadata.get(task_name)
        raw_data = load_data(task_name)

        show_statistics(raw_data, task_name, crawl_time)


if __name__ == "__main__":
    main()