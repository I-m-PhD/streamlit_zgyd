# app.py
# æ­¤æ–‡ä»¶ç”¨äºæ‰‹åŠ¨é‡‡é›†å‰ä¸¤ä¸ªæ•°æ®é›†

import streamlit as st
import random, requests, ssl, time, json, os
from requests.adapters import HTTPAdapter
import pandas as pd
import plotly.express as px
import plotly.io as pio
from datetime import datetime

# --- GLOBAL SETTINGS AND UTILITIES ---

# --- API Endpoints ---
BASE_URL = 'https://b2b.10086.cn'
POST_URL = f'{BASE_URL}/api-b2b/api-sync-es/white_list_api/b2b/publish/queryList'
# ä½¿ç”¨å›ºå®šçš„æœ¬åœ°æ–‡ä»¶å¤¹æ¥å­˜å‚¨æ•°æ®
OUTPUT_DIR = "./zgyd"

# å®šä¹‰æ‰€æœ‰éœ€è¦é‡‡é›†çš„ä»»åŠ¡é…ç½®
TASK_CONFIG = {
    "æ‰€æœ‰æ‹›é‡‡": {"payload": {}, "name": "æ‰€æœ‰æ‹›é‡‡"},
    "æ­£åœ¨æ‹›æ ‡": {"payload": {"homePageQueryType": "Bidding"}, "name": "æ‰€æœ‰æ‹›é‡‡_æ­£åœ¨æ‹›æ ‡"},
    "æ­£åœ¨æ‹›æ ‡ (åŒ—äº¬)": {"payload": {"homePageQueryType": "Bidding", "companyType": "BJ"}, "name": "æ‰€æœ‰æ‹›é‡‡_æ­£åœ¨æ‹›æ ‡_åŒ—äº¬"},
}

# ç”¨äºè®°å½•æ•°æ®é‡‡é›†æ—¶é—´çš„æ–‡ä»¶
METADATA_PATH = os.path.join(OUTPUT_DIR, "metadata.json")

# --- User Agents (Keeping the mechanism but truncating list display) ---
USER_AGENTS = [
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.6 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; HTC; Titan)",
    "MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)",
] * 10

def get_random_headers():
    """Generates standard headers with a random User-Agent."""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'application/json, text/plain, */*',
        'Content-Type': 'application/json',
        'Origin': f'{BASE_URL}',
        'Referer': f'{BASE_URL}/',
    }

class CustomHttpAdapter(HTTPAdapter):
    """HTTPAdapter that handles the required SSL context fix."""
    def init_poolmanager(self, *args, **kwargs):
        context = ssl.create_default_context()
        context.options |= 0x4
        context.check_hostname = False
        kwargs['ssl_context'] = context
        return super(CustomHttpAdapter, self).init_poolmanager(*args, **kwargs)

# --- METADATA HANDLER ---

def load_metadata():
    """Loads the last successful crawl time for all tasks."""
    if os.path.exists(METADATA_PATH):
        try:
            with open(METADATA_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            # æ–‡ä»¶æŸåæˆ–ä¸ºç©ºï¼Œè¿”å›ç©ºå­—å…¸
            return {}
    return {}

def save_metadata(metadata):
    """Saves the last successful crawl time."""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(METADATA_PATH, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)

# --- DATA SCRAPING FUNCTION (Writing to Disk) ---

def scrape_content(payload_override, output_name, status_placeholder):
    """
    Scrapes data and saves it to a local JSON file. (å’Œ crawler.py ä¸­ä¿æŒä¸€è‡´)
    """

    # Base Payload structure (defaults)
    base_payload = {
        "size": 100, "current": 1, "companyType": "", "name": "",
        "publishType": "PROCUREMENT", "publishOneType": "PROCUREMENT",
        "homePageQueryType": "", "sfactApplColumn5": "PC"
    }

    payload = base_payload.copy()
    payload.update(payload_override)
    page_size = payload['size']
    current_page = payload['current']
    all_content = []

    session = requests.Session()
    session.mount('https://', CustomHttpAdapter())

    status_placeholder.info(f"[{output_name}] ğŸš€ å¼€å§‹æŠ“å–æ•°æ®...")

    success = True
    while True:
        payload['current'] = current_page
        headers = get_random_headers()

        try:
            status_placeholder.text(f"[{output_name}] æ­£åœ¨æŠ“å–ç¬¬ {current_page} é¡µï¼Œå½“å‰æ€»è®¡ {len(all_content)} æ¡è®°å½•...")
            response = session.post(POST_URL, headers=headers, json=payload, timeout=15)
            response.raise_for_status()
            response_json = response.json()

            page_content = response_json.get('data', {}).get('content', [])
            content_count = len(page_content)

            if not page_content:
                status_placeholder.text(f"[{output_name}] ç¬¬ {current_page} é¡µæ— å†…å®¹ã€‚æŠ“å–åœæ­¢ã€‚")
                break

            all_content.extend(page_content)

            if content_count < page_size:
                status_placeholder.text(f"[{output_name}] å·²è¾¾åˆ°æœ€åä¸€é¡µã€‚æ€»è®¡ {len(all_content)} æ¡è®°å½•ã€‚")
                break

            current_page += 1
            time.sleep(random.uniform(2, 5))

        except requests.exceptions.RequestException as e:
            status_placeholder.error(f"[{output_name}] âš ï¸ è¯·æ±‚ç¬¬ {current_page} é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            success = False
            time.sleep(10)
            break
        except json.JSONDecodeError:
            status_placeholder.error(f"[{output_name}] âš ï¸ æ— æ³•è§£æç¬¬ {current_page} é¡µçš„ JSON å“åº”ã€‚")
            success = False
            time.sleep(10)
            break

    final_count = len(all_content)

    # --- Saving Data ---
    if success and final_count > 0:
        output_path = os.path.join(OUTPUT_DIR, f"{output_name}.json")
        try:
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(all_content, f, indent=4, ensure_ascii=False)
            status_placeholder.success(f"[{output_name}] âœ… æŠ“å–å®Œæˆã€‚æ€»è®¡è®°å½•: **{final_count}** æ¡ã€‚")
            return True
        except Exception as e:
            status_placeholder.error(f"[{output_name}] âŒ å†™å…¥ JSON æ–‡ä»¶å¤±è´¥: {e}")
            return False
    elif final_count == 0:
        status_placeholder.warning(f"[{output_name}] æœªè·å–åˆ°ä»»ä½•è®°å½•ã€‚")
        return False
    else:
        return False


# --- DATA LOADING (Reading from Disk) ---

def load_data(task_name):
    """Loads data from a local JSON file."""
    output_path = os.path.join(OUTPUT_DIR, f"{task_name}.json")
    if os.path.exists(output_path):
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return None
    return None

# --- DATA ANALYSIS AND PLOTTING FUNCTION ---

# Setting plotly template
pio.templates.default = "plotly_dark"

def show_statistics(all_content, data_name, crawl_time, is_manual_task=True): # å¢åŠ  is_manual_task å‚æ•°

    st.markdown("---")
    st.header(f"æ•°æ®åˆ†æ: {data_name}")

    # --- æ–°å¢/ä¿®æ”¹ï¼šæ‰‹åŠ¨é‡‡é›†æŒ‰é’®å’ŒçŠ¶æ€æ˜¾ç¤º ---
    if is_manual_task:
        st.subheader("æ‰‹åŠ¨é‡‡é›†æ§åˆ¶")
        task_config = next(v for k, v in TASK_CONFIG.items() if v["name"] == data_name)

        # æŒ‰é’®å®¹å™¨
        col_btn, col_status = st.columns([1, 4])

        if col_btn.button(f"ç°åœ¨é‡‡é›†: {data_name}", key=f"btn_{data_name}", type="primary"):
            start_time = datetime.now()

            # ä½¿ç”¨çŠ¶æ€å ä½ç¬¦
            status_placeholder = col_status.empty()

            success = scrape_content(task_config["payload"], data_name, status_placeholder)

            if success:
                metadata = load_metadata()
                metadata[data_name] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                save_metadata(metadata)

            status_placeholder.empty()
            col_status.success(f"é‡‡é›†å®Œæˆï¼è€—æ—¶: {datetime.now() - start_time}ã€‚è¯·åˆ·æ–°é¡µé¢æŸ¥çœ‹æ–°æ•°æ®ã€‚")
            st.rerun() # è§¦å‘é¡µé¢é‡æ–°åŠ è½½ä»¥æ˜¾ç¤ºæ–°æ•°æ®

        else:
            col_status.caption(f"ä¸Šæ¬¡é‡‡é›†æ—¶é—´ï¼š**{crawl_time if crawl_time else 'æ— è®°å½•'}**")

    else:
        # è‡ªåŠ¨é‡‡é›†ä»»åŠ¡çš„æ˜¾ç¤ºé€»è¾‘
        st.subheader("è‡ªåŠ¨é‡‡é›†çŠ¶æ€")
        if crawl_time:
            st.caption(f"æ•°æ®é‡‡é›†æ—¶é—´ï¼š**{crawl_time}** (ç”± GitHub Action å®šæ—¶æ›´æ–°)")
        else:
            st.caption("å°šæœªæˆåŠŸé‡‡é›†è¯¥ä»»åŠ¡æ•°æ®ã€‚æ•°æ®ç”± GitHub Action å®šæ—¶æ›´æ–°ã€‚")

    st.markdown("---") # ç¡®ä¿å›¾è¡¨å’ŒæŒ‰é’®/çŠ¶æ€åˆ†ç¦»

    if not all_content:
        st.warning("æ— æ•°æ®å¯ä¾›åˆ†æã€‚")
        return

    df = pd.DataFrame(all_content)

    # ... (ä¿æŒåŸæœ‰çš„æ—¥æœŸè½¬æ¢ã€è¿‡æ»¤ã€ç‰¹å¾å·¥ç¨‹é€»è¾‘) ...
    if df.empty or 'publishDate' not in df.columns:
        st.warning("DataFrame ä¸ºç©ºæˆ–ç¼ºå°‘ 'publishDate' å­—æ®µã€‚æ— æ³•ç”Ÿæˆå›¾è¡¨ã€‚")
        return

    try:
        # ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning
        df['PublishDateTime'] = pd.to_datetime(df['publishDate'])
    except Exception as e:
        st.error(f"æ—¥æœŸæ ¼å¼è½¬æ¢é”™è¯¯: {e}")
        return

    cutoff_date = pd.to_datetime('2024-01-01')
    initial_count = len(df)
    df = df[df['PublishDateTime'] >= cutoff_date].copy() # å†æ¬¡ä½¿ç”¨ copy() ç¡®ä¿é“¾å¼æ“ä½œå®‰å…¨
    filtered_count = len(df)

    if initial_count != filtered_count:
        st.info(f"å·²è¿‡æ»¤ {initial_count - filtered_count} æ¡æ—©äº {cutoff_date.date()} çš„å†å²å™ªéŸ³è®°å½•ã€‚")

    df['PublishDateOnly'] = df['PublishDateTime'].dt.date
    df['PublishHour'] = df['PublishDateTime'].dt.hour
    df['PublishDayOfWeek'] = df['PublishDateTime'].dt.day_name(locale='en_GB')
    day_map = {'Monday': 'å‘¨ä¸€', 'Tuesday': 'å‘¨äºŒ', 'Wednesday': 'å‘¨ä¸‰', 'Thursday': 'å‘¨å››', 'Friday': 'å‘¨äº”', 'Saturday': 'å‘¨å…­', 'Sunday': 'å‘¨æ—¥'}
    df['PublishDayOfWeek'] = df['PublishDayOfWeek'].map(day_map)
    day_order = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
    df['PublishDayOfWeek'] = pd.Categorical(df['PublishDayOfWeek'], categories=day_order, ordered=True)

    # --- Plotting Logic (ä¿æŒä¸å˜) ---
    st.subheader("1. æ¯æ—¥æ›´æ–°é¢‘æ¬¡")
    # ... (Plotting code for PLOT 1) ...
    frequency_df = df.groupby(['PublishDateOnly', 'PublishDayOfWeek'], observed=True)['PublishDateTime'].count().reset_index()
    frequency_df.columns = ['PublishDate', 'PublishDayOfWeek', 'UpdateCount']
    frequency_df = frequency_df.sort_values('PublishDate')

    fig_freq = px.bar(
        frequency_df, x='PublishDate', y='UpdateCount', title='æ¯æ—¥æ›´æ–°é¢‘æ¬¡ (å¯ç¼©æ”¾)',
        labels={'UpdateCount': 'æ›´æ–°é¢‘æ¬¡', 'PublishDate': 'æ—¥æœŸ', 'PublishDayOfWeek': 'å‘¨å‡ '},
        hover_data=['PublishDayOfWeek'], height=500
    )
    fig_freq.update_xaxes(
        tickangle=-45, rangeslider_visible=True,
        rangeselector=dict(
            bgcolor="#333333", activecolor="#555555", font=dict(color="white"),
            buttons=[
                dict(count=7, label="1å‘¨", step="day", stepmode="backward"),
                dict(count=1, label="1æœˆ", step="month", stepmode="backward"),
                dict(count=3, label="1å­£", step="month", stepmode="backward"),
                dict(count=1, label="1å¹´", step="year", stepmode="backward"),
                dict(step="all", label="å…¨éƒ¨")
            ]),
        tickformat="%Y-%m-%d"
    )
    st.plotly_chart(fig_freq, use_container_width=True)

    st.subheader("2. æ›´æ–°æ´»è·ƒåº¦åˆ†æ")
    col1, col2 = st.columns(2)
    with col1:
        time_df_hour = df.groupby('PublishHour', observed=True)['PublishDateTime'].count().reset_index(name='UpdateCount')
        fig_hour = px.bar(time_df_hour, x='PublishHour', y='UpdateCount', title='å…¨æ—¶æ®µæ›´æ–°æ´»è·ƒåº¦', labels={'PublishHour': 'æ—¶åˆ» (0-23)', 'UpdateCount': 'æ›´æ–°é¢‘æ¬¡'}, height=400)
        fig_hour.update_layout(xaxis={'tickmode': 'linear', 'dtick': 1})
        st.plotly_chart(fig_hour, use_container_width=True)
    with col2:
        hour_order = list(range(24))
        time_df_heatmap = df.groupby(['PublishHour', 'PublishDayOfWeek'], observed=True).size().reset_index(name='UpdateCount')
        index = pd.MultiIndex.from_product([hour_order, day_order], names=['PublishHour', 'PublishDayOfWeek'])
        time_df_heatmap = time_df_heatmap.set_index(['PublishHour', 'PublishDayOfWeek']).reindex(index, fill_value=0).reset_index()
        time_df_heatmap['PublishDayOfWeek'] = pd.Categorical(time_df_heatmap['PublishDayOfWeek'], categories=day_order, ordered=True)
        fig_heatmap = px.density_heatmap(time_df_heatmap, x="PublishHour", y="PublishDayOfWeek", z="UpdateCount", title='æ›´æ–°æ´»è·ƒåº¦: æ—¶åˆ» vs. å‘¨å‡ ', labels={"PublishHour": "æ—¶åˆ»", "PublishDayOfWeek": "å‘¨å‡ ", "UpdateCount": "æ›´æ–°é¢‘æ¬¡"}, category_orders={"PublishDayOfWeek": day_order, "PublishHour": hour_order}, nbinsx=24, color_continuous_scale=px.colors.sequential.Viridis, height=400)
        fig_heatmap.update_xaxes(range=[-0.5, 23.5], tickmode='linear', dtick=1)
        st.plotly_chart(fig_heatmap, use_container_width=True)


    # --- 3. åŸå§‹æ•°æ®è¡¨æ ¼ (ä»…é™åŒ—äº¬) ---
    if data_name == "æ‰€æœ‰æ‹›é‡‡_æ­£åœ¨æ‹›æ ‡_åŒ—äº¬":
        st.subheader("3. åŸå§‹æ•°æ®è¡¨")

        required_cols_map = {
            'publishDate': 'å‘å¸ƒæ—¶é—´',
            'companyTypeName': 'å…¬å¸åŒºåŸŸ',
            'name': 'æ ‡é¢˜',
            'tenderSaleDeadline': 'æˆªæ ‡æ—¶é—´',
            'backDate': 'é€€å›æ—¶é—´'
        }

        available_cols = [col for col in required_cols_map.keys() if col in df.columns]

        if not available_cols:
            st.warning("æ— æ³•æ˜¾ç¤ºæ•°æ®è¡¨ï¼šæŠ“å–çš„æ•°æ®ä¸­ç¼ºå°‘å¿…è¦çš„å­—æ®µã€‚")
            return

        rename_map = {col: required_cols_map[col] for col in available_cols}
        display_df = df[available_cols].rename(columns=rename_map)

        if 'å‘å¸ƒæ—¶é—´' in display_df.columns:
            display_df = display_df.sort_values(by='å‘å¸ƒæ—¶é—´', ascending=False)

        st.dataframe(display_df, use_container_width=True, height=600)


# --- MAIN APPLICATION ENTRY POINT ---

def main():
    st.set_page_config(
        page_title="æ‹›é‡‡æ•°æ®ç›‘æ§",
        layout="wide",
        initial_sidebar_state="collapsed"
    )

    st.title("ğŸ“¡ ä¸­å›½ç§»åŠ¨æ‹›é‡‡å¹³å°æ•°æ®ç›‘æ§")
    st.info("æ•°æ®é›† 1 å’Œ 2 å¯æ‰‹åŠ¨è§¦å‘é‡‡é›†ï¼›æ•°æ®é›† 3 ç”± GitHub Actions å®šæ—¶æ›´æ–°ã€‚")

    # 4. ä¼˜å…ˆè¯»å–æœ¬åœ° json æ–‡ä»¶å¹¶æ˜¾ç¤ºå›¾è¡¨
    metadata = load_metadata()

    # éå†æ‰€æœ‰ä»»åŠ¡é…ç½®
    for i, task_key in enumerate(TASK_CONFIG.keys()):
        config = TASK_CONFIG[task_key]
        task_name = config["name"]

        crawl_time = metadata.get(task_name)
        raw_data = load_data(task_name)

        # åˆ¤æ–­æ˜¯å¦ä¸ºæ‰‹åŠ¨ä»»åŠ¡
        is_manual = (i < 2) # å‰ä¸¤ä¸ªä»»åŠ¡ (ç´¢å¼• 0 å’Œ 1) æ˜¯æ‰‹åŠ¨çš„

        # ä¼ å…¥ is_manual å‚æ•°æ¥æ§åˆ¶æŒ‰é’®å’ŒçŠ¶æ€çš„æ˜¾ç¤º
        show_statistics(raw_data, task_name, crawl_time, is_manual_task=is_manual)


if __name__ == "__main__":
    main()