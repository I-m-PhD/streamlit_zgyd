# .github/workflows/zgyd_scheduler.yml
# 此文件确保每 30 分钟触发 zgyd_crawler.py

name: Scheduled Data Crawler

on:
  # 定时任务：每 30 分钟执行一次 (每小时的 0 分和 30 分)
  schedule:
    - cron: '0,30 * * * *'
  # 允许手动触发
  workflow_dispatch:

jobs:
  crawl_job:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13.5'

      - name: Install dependencies
        run: |
          # 安装 requirements.txt 中列出的所有依赖
          pip install -r requirements.txt 

      - name: Run Crawler Script
        run: python zgyd_crawler.py

      - name: Commit and Push new data
        # 使用 git-auto-commit-action 来自动提交更新的文件
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "SCHEDULER: Update '正在招标 (北京)' data via Action."
          # 仅提交更新的 JSON 文件和 metadata 文件
          file_pattern: 'zgyd/*.json zgyd/metadata.json'
          # 设置提交用户和邮箱，避免权限问题
          commit_user_name: 'github-actions[bot]'
          commit_user_email: 'github-actions[bot]@users.noreply.github.com'